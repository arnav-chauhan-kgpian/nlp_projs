{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b3GxjsfMYP38"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d prashantkarwasra/books-dataset-text-generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBxrjT4eZ3jS",
        "outputId": "99291487-ac48-49e2-e703-93cc313c0d0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/prashantkarwasra/books-dataset-text-generation\n",
            "License(s): unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"books-dataset-text-generation.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6q4nfmzawxr",
        "outputId": "192d9ec3-8c96-4041-9f7b-56dab70cd94a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  books-dataset-text-generation.zip\n",
            "  inflating: /content/dataset/HarryPotter1.txt  \n",
            "  inflating: /content/dataset/HarryPotter2.txt  \n",
            "  inflating: /content/dataset/HarryPotter3.txt  \n",
            "  inflating: /content/dataset/HarryPotter4.txt  \n",
            "  inflating: /content/dataset/HarryPotter5.txt  \n",
            "  inflating: /content/dataset/HarryPotter6.txt  \n",
            "  inflating: /content/dataset/HarryPotter7.txt  \n",
            "  inflating: /content/dataset/Hobbit1.txt  \n",
            "  inflating: /content/dataset/LOTR1.txt  \n",
            "  inflating: /content/dataset/LOTR2.txt  \n",
            "  inflating: /content/dataset/LOTR3.txt  \n",
            "  inflating: /content/dataset/Silmarillion4.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files_to_delete = ['dataset/HarryPotter1.txt','dataset/HarryPotter2.txt','dataset/HarryPotter3.txt','dataset/HarryPotter4.txt','dataset/HarryPotter5.txt','dataset/HarryPotter6.txt'\n",
        ",'dataset/HarryPotter7.txt', 'dataset/Silmarillion4.txt']\n",
        "\n",
        "for file in files_to_delete:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "        print(f\"Deleted: {file}\")\n",
        "    else:\n",
        "        print(f\"File not found: {file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8vsZwLTa9fl",
        "outputId": "bd06a9a7-7751-4c36-d998-3fe46341fd2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: dataset/HarryPotter1.txt\n",
            "Deleted: dataset/HarryPotter2.txt\n",
            "Deleted: dataset/HarryPotter3.txt\n",
            "Deleted: dataset/HarryPotter4.txt\n",
            "Deleted: dataset/HarryPotter5.txt\n",
            "Deleted: dataset/HarryPotter6.txt\n",
            "Deleted: dataset/HarryPotter7.txt\n",
            "Deleted: dataset/Silmarillion4.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time"
      ],
      "metadata": {
        "id": "XY-IgOZRbLIq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = ['dataset/Hobbit1.txt','dataset/LOTR1.txt','dataset/LOTR2.txt','dataset/LOTR3.txt']\n",
        "with open('LOTR.txt', 'w') as outfile:\n",
        "  for file in files:\n",
        "    with open(file) as infile:\n",
        "      outfile.write(infile.read())\n",
        "\n",
        "text = open('LOTR.txt').read()\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpTL6D-obhqz",
        "outputId": "0298fac4-e3b6-452e-ebf2-50468c6f6fb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 3041991 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs_C5vslb3ZR",
        "outputId": "99964fa0-6307-4c06-b258-02b9a1cbae70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter I \n",
            "\n",
            "\n",
            "AN UNEXPECTED PARTY \n",
            "\n",
            "\n",
            "In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms \n",
            "and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a \n",
            "hobbit-hole, and that means comfort. \n",
            "\n",
            "It had a per\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bZObSCmcRfo",
        "outputId": "012ac58f-98e7-4296-ff83-2fa6e46f34b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2index = {u:i for i, u in enumerate(vocab)}\n",
        "index2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2index[c] for c in text])\n",
        "\n",
        "print(text_as_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBXXCTvBcfbm",
        "outputId": "4dbfad76-d301-4fc4-bf6b-66e55549ecf6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30 63 56 ... 64 59 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show how the first 30 characters from the text are mapped to integers\n",
        "print ('{} -- characters mapped to int -- > {}'.format(repr(text[:30]), text_as_int[:30]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cENQgPd8dH13",
        "outputId": "67c3d5db-0f58-4a34-d17b-483c9088f57a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Chapter I \\n\\n\\nAN UNEXPECTED PAR' -- characters mapped to int -- > [30 63 56 71 75 60 73  2 36  2  1  1  1 28 41  2 48 41 32 51 43 32 30 47\n",
            " 32 31  2 43 28 45]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(30):\n",
        "  print(index2char[i.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFE8Vz3Odj92",
        "outputId": "93081000-ea9e-43d3-d3f6-3e4ad5b2e2b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C\n",
            "h\n",
            "a\n",
            "p\n",
            "t\n",
            "e\n",
            "r\n",
            " \n",
            "I\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A\n",
            "N\n",
            " \n",
            "U\n",
            "N\n",
            "E\n",
            "X\n",
            "P\n",
            "E\n",
            "C\n",
            "T\n",
            "E\n",
            "D\n",
            " \n",
            "P\n",
            "A\n",
            "R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(index2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZwIDmHJeU27",
        "outputId": "6a04def3-89ac-4790-8cf0-a5bcede3e6a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Chapter I \\n\\n\\nAN UNEXPECTED PARTY \\n\\n\\nIn a hole in the ground there lived a hobbit. Not a nasty, dirty,'\n",
            "' wet hole, filled with the ends of worms \\nand an oozy smell, nor yet a dry, bare, sandy hole with not'\n",
            "'hing in it to sit down on or to eat: it was a \\nhobbit-hole, and that means comfort. \\n\\nIt had a perfec'\n",
            "'tly round door like a porthole, painted green, with a shiny yellow brass knob in the \\nexact middle. T'\n",
            "'he door opened on to a tube-shaped hall like a tunnel: a very comfortable tunnel \\nwithout smoke, with'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "WHIELUcWeMzh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka5DRGGFekGO",
        "outputId": "a201e039-4a49-4bd5-fcd2-850343d3bae4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 300\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units1 = 1024\n",
        "rnn_units2 = 1024\n",
        "rnn_units=[rnn_units1, rnn_units2]\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnx283yser4P",
        "outputId": "7fbf00fc-069d-4e7e-eaaf-76946bad0978"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, dropout_rate=0.2):\n",
        "    rnn_units1, rnn_units2 = rnn_units  # Unpack the units\n",
        "\n",
        "    # Input layer\n",
        "    inputs = tf.keras.Input(batch_shape=(batch_size, None), name=\"input\")\n",
        "\n",
        "    # Learnable positional embeddings\n",
        "    position_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=1000,  # Max sequence length you expect\n",
        "        output_dim=embedding_dim,\n",
        "        name=\"positional_embedding\"\n",
        "    )\n",
        "\n",
        "    token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        name=\"token_embedding\"\n",
        "    )\n",
        "\n",
        "    def add_position_embedding(x):\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "        positions = position_embedding(positions)\n",
        "        return x + positions\n",
        "\n",
        "    x = token_embedding(inputs)\n",
        "    x = tf.keras.layers.Lambda(add_position_embedding, name=\"add_pos_embedding\")(x)\n",
        "\n",
        "    # First GRU + LayerNorm + Dropout\n",
        "    x1 = tf.keras.layers.GRU(\n",
        "        rnn_units1,\n",
        "        return_sequences=True,\n",
        "        stateful=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        name=\"gru_1\"\n",
        "    )(x)\n",
        "\n",
        "    x1 = tf.keras.layers.LayerNormalization(name=\"ln_1\")(x1)\n",
        "    x1 = tf.keras.layers.Dropout(dropout_rate)(x1)\n",
        "\n",
        "    # Second GRU (with residual connection if dimensions match)\n",
        "    x2 = tf.keras.layers.GRU(\n",
        "        rnn_units2,\n",
        "        return_sequences=True,\n",
        "        stateful=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        name=\"gru_2\"\n",
        "    )(x1)\n",
        "\n",
        "    # Optional: Residual connection if rnn_units1 == rnn_units2\n",
        "    if rnn_units1 == rnn_units2:\n",
        "        x2 = tf.keras.layers.Add(name=\"residual_add\")([x1, x2])\n",
        "\n",
        "    x2 = tf.keras.layers.LayerNormalization(name=\"ln_2\")(x2)\n",
        "    x2 = tf.keras.layers.Dropout(dropout_rate)(x2)\n",
        "\n",
        "    # Output projection\n",
        "    outputs = tf.keras.layers.Dense(vocab_size, name=\"output_logits\")(x2)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs, name=\"TextGenGRU_SOTA_Lite\")\n"
      ],
      "metadata": {
        "id": "6SrWRhNKe3i0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "dJCYevr_e73t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "wjzNEYoMe-Bk",
        "outputId": "78614d49-9794-4bdb-b04d-6da3e3bbdb4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"TextGenGRU_SOTA_Lite\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TextGenGRU_SOTA_Lite\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_embedding     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │     \u001b[38;5;34m26,100\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_pos_embedding   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │  \u001b[38;5;34m4,073,472\u001b[0m │ add_pos_embeddin… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_1                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │      \u001b[38;5;34m2,048\u001b[0m │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ ln_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │  \u001b[38;5;34m6,297,600\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_add (\u001b[38;5;33mAdd\u001b[0m)  │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │                   │            │ gru_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_2                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │      \u001b[38;5;34m2,048\u001b[0m │ residual_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ ln_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_logits       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m87\u001b[0m)    │     \u001b[38;5;34m89,175\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_embedding     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">26,100</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_pos_embedding   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,073,472</span> │ add_pos_embeddin… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_1                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ln_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,297,600</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │                   │            │ gru_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_2                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ residual_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ln_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_logits       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">89,175</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,490,443\u001b[0m (40.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,490,443</span> (40.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,490,443\u001b[0m (40.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,490,443</span> (40.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "Y49XRpr4fTdS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mKHrRgqIi2Dd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Directory to save the best model weights\n",
        "checkpoint_dir = './training_checkpoints_final'\n",
        "os.makedirs(checkpoint_dir,exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'best_model_final.weights.h5')\n",
        "\n",
        "# Create the callback\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "rmPjqSsXi7K8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=25, callbacks=checkpoint_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvanj4N3i935",
        "outputId": "90b4692c-6adc-4149-8459-1b37c6142793"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 144ms/step - accuracy: 0.3540 - loss: 2.6673\n",
            "Epoch 2/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 153ms/step - accuracy: 0.5533 - loss: 1.5162\n",
            "Epoch 3/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 152ms/step - accuracy: 0.5904 - loss: 1.3708\n",
            "Epoch 4/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 153ms/step - accuracy: 0.6086 - loss: 1.2981\n",
            "Epoch 5/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 153ms/step - accuracy: 0.6198 - loss: 1.2544\n",
            "Epoch 6/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 153ms/step - accuracy: 0.6275 - loss: 1.2230\n",
            "Epoch 7/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 152ms/step - accuracy: 0.6341 - loss: 1.1992\n",
            "Epoch 8/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 153ms/step - accuracy: 0.6388 - loss: 1.1804\n",
            "Epoch 9/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - accuracy: 0.6427 - loss: 1.1645\n",
            "Epoch 10/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 152ms/step - accuracy: 0.6465 - loss: 1.1502\n",
            "Epoch 11/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 152ms/step - accuracy: 0.6497 - loss: 1.1383\n",
            "Epoch 12/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 153ms/step - accuracy: 0.6522 - loss: 1.1280\n",
            "Epoch 13/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - accuracy: 0.6548 - loss: 1.1197\n",
            "Epoch 14/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 153ms/step - accuracy: 0.6579 - loss: 1.1074\n",
            "Epoch 15/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 152ms/step - accuracy: 0.6602 - loss: 1.0993\n",
            "Epoch 16/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 153ms/step - accuracy: 0.6622 - loss: 1.0915\n",
            "Epoch 17/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - accuracy: 0.6648 - loss: 1.0826\n",
            "Epoch 18/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 152ms/step - accuracy: 0.6663 - loss: 1.0755\n",
            "Epoch 19/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - accuracy: 0.6686 - loss: 1.0688\n",
            "Epoch 20/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - accuracy: 0.6704 - loss: 1.0617\n",
            "Epoch 21/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 154ms/step - accuracy: 0.6718 - loss: 1.0560\n",
            "Epoch 22/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 153ms/step - accuracy: 0.6725 - loss: 1.0521\n",
            "Epoch 23/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 154ms/step - accuracy: 0.6737 - loss: 1.0492\n",
            "Epoch 24/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 154ms/step - accuracy: 0.6752 - loss: 1.0432\n",
            "Epoch 25/25\n",
            "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 154ms/step - accuracy: 0.6767 - loss: 1.0386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latest_check= 'training_checkpoints_final/best_model_final.weights.h5'"
      ],
      "metadata": {
        "id": "80crW1zjmif4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(latest_check)\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "6xEPvi2rm6vk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "I6vkPo23nABD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "0f99f3bd-499f-4b94-a90d-cfa692d903f4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"TextGenGRU_SOTA_Lite\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TextGenGRU_SOTA_Lite\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_embedding     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)    │     \u001b[38;5;34m26,100\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_pos_embedding   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │  \u001b[38;5;34m4,073,472\u001b[0m │ add_pos_embeddin… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_1                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │      \u001b[38;5;34m2,048\u001b[0m │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ ln_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │  \u001b[38;5;34m6,297,600\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_add (\u001b[38;5;33mAdd\u001b[0m)  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ gru_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_2                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │      \u001b[38;5;34m2,048\u001b[0m │ residual_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ ln_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_logits       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m87\u001b[0m)     │     \u001b[38;5;34m89,175\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_embedding     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">26,100</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_pos_embedding   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,073,472</span> │ add_pos_embeddin… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_1                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ln_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,297,600</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ gru_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_2                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ residual_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ln_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_logits       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">89,175</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,490,443\u001b[0m (40.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,490,443</span> (40.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,490,443\u001b[0m (40.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,490,443</span> (40.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2index[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low results in more predictable text.\n",
        "  # Higher results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  scaling = 0.5 #1\n",
        "\n",
        "  # batch size == 1\n",
        "\n",
        "  #model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / scaling\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(index2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "OkrT36donClN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"Hobbiton \"))"
      ],
      "metadata": {
        "id": "eLfZnOHcnzVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8646b1-4729-4d2d-a81e-3df4b4f0b762"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hobbiton the Great Gate of the Elves, and their horses were grim and surrounding. They were remembered that the sound of his people came and the winding of the stream was still sung in the sover of the dark hour, and soon too had been to bow him out of the fields of the Houses of Healing.\n",
            "     'The evening of the Ringwraiths have been set and see it any long time to be looking for them to this old and the branch to see the Lord of the Mark!' said Aragorn. 'We must go and go that and be a fool or day before the ring of the village.'\n",
            "     'I wonder if he's come to phanted you, and I say you have no heart to ask for the most likely to think of that and all the time and hear.' He said now and again to the road after the fallen stone, and found his master in his time and a slender fire in the sunlight. 'We must go and say to Gollum's eyes on your way to Minas Tirith, and they mean to find a long sure of a wide light of their silver and mountain, and then suddenly in the sunlight of the day's march w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"The ring \"))"
      ],
      "metadata": {
        "id": "Nr8xofsXoBF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3710f4-3d0a-4e4c-9e44-728fb92d0109"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ring is our father to another to the point of the world outside the Lord of the Mountain, and a little of our horses and the others built in the Morgai that was not a horse in the Shire. So he said: \"Has they too mate any song of your own fools who has done much to see it in your business to the Sackville-Bagginses an ancient strangers. And the doom of the messenger of the Ring was one of the black shadow of all that the sound of the woods of Mordor, and where the day the beauty is not a horse of wine that was broken or something to warm another coming of any more than a little hope of his own wisdom untied to him of that land. In the meanwhile he was to keep a wind of a great horse and some of it almost to the face of the stones. But if you don't like to see that he would have been a fool or horse that he will see that he has been to begin to speak of it again. And if it were one of the black shadow of all that you used to be slain and some wise and heartening to our hope in the morning _h\n"
          ]
        }
      ]
    }
  ]
}